import os
import time
import gradio as gr
import json
from demo import create_llm_chain
from dotenv import load_dotenv

load_dotenv()

title = """<h1 align="center">Kepler - An AI Assistant that Orbits Around You üöÄ</h1>"""

CHAT_HISTORY = []

LLM_CHAIN = create_llm_chain(
    model_name=os.getenv("LLM_NAME"),
    temperature=os.getenv("LLM_TEMPERATURE"),
    max_tokens=os.getenv("LLM_MAX_TOKENS"),
    index_path=os.getenv("FAISS_INDEX_PATH"),
    request_timeout=os.getenv("LLM_REQUEST_TIMEOUT"),
    max_retries=os.getenv("LLM_MAX_RETRIES"),
)

CURRENT_SETTINGS = {
    "model_name": os.getenv("LLM_NAME"),
    "temperature": os.getenv("LLM_TEMPERATURE"),
    "max_tokens": os.getenv("LLM_MAX_TOKENS"),
    "timeout": os.getenv("LLM_REQUEST_TIMEOUT"),
    "max_retries": os.getenv("LLM_MAX_RETRIES"),
}


def inference(message, chat_history_app):
    try:
        result = LLM_CHAIN({"question": message, "chat_history": CHAT_HISTORY})
    except Exception as e:
        chat_history_app.append((message, "ERROR OCCURED"))
        return "", chat_history_app, e

    CHAT_HISTORY.append((message, result["answer"]))
    chat_history_app.append((message, result["answer"]))

    return "", chat_history_app, "SUCCESS"


def reset_llm_chain(
    model_name, temperature, max_tokens, timeout_seconds, max_retries, defaults=False
):
    global LLM_CHAIN
    global CURRENT_SETTINGS

    if defaults:
        CURRENT_SETTINGS["model_name"] = os.getenv("LLM_NAME")
        CURRENT_SETTINGS["temperature"] = os.getenv("LLM_TEMPERATURE")
        CURRENT_SETTINGS["max_tokens"] = os.getenv("LLM_MAX_TOKENS")
        CURRENT_SETTINGS["timeout"] = os.getenv("LLM_REQUEST_TIMEOUT")
        CURRENT_SETTINGS["max_retries"] = os.getenv("LLM_MAX_RETRIES")
    else:
        CURRENT_SETTINGS["model_name"] = model_name
        CURRENT_SETTINGS["temperature"] = temperature
        CURRENT_SETTINGS["max_tokens"] = max_tokens
        CURRENT_SETTINGS["timeout"] = timeout_seconds
        CURRENT_SETTINGS["max_retries"] = max_retries

    LLM_CHAIN = create_llm_chain(
        model_name=CURRENT_SETTINGS["model_name"],
        temperature=CURRENT_SETTINGS["temperature"],
        max_tokens=CURRENT_SETTINGS["max_tokens"],
        index_path=os.getenv("FAISS_INDEX_PATH"),
        request_timeout=CURRENT_SETTINGS["timeout"],
        max_retries=CURRENT_SETTINGS["max_retries"],
    )

    return json.dumps(CURRENT_SETTINGS)


def clear_chat_history():
    global CHAT_HISTORY
    CHAT_HISTORY = []


with gr.Blocks(title="Kepler") as demo:
    gr.HTML(title)
    gr.HTML(
        """<h3 align="center"> ‚ö†Ô∏è This is a demo version! Be sure to use the settings tab wisely. </h2>"""
    )
    gr.HTML(
        """<h3 align="center"> üí°This bot only has knowledge on the certain documents, contact Abhinand to learn more. </h2>"""
    )

    with gr.Tab("Chat Interface"):
        chatbot = gr.Chatbot(label="Kepler").style(height=600)
        msg = gr.Textbox(label="Send a message.")
        clear = gr.Button("Clear")
        status_code = gr.Textbox(
            label="Message from OpenAI",
        )

        msg.submit(inference, [msg, chatbot], [msg, chatbot, status_code])
        clear.click(clear_chat_history, None, chatbot, queue=False)

    with gr.Tab("Settings"):
        gr.HTML(
            """<h4 align="center"> ‚ö†Ô∏è WARNING: Reloading will revert these settings to their defaults! </h4>"""
        )

        current_settings = gr.Textbox(label="Current Settings")
        current_settings.value = json.dumps(CURRENT_SETTINGS)

        temperature = gr.Slider(
            minimum=-0,
            maximum=1.0,
            step=0.05,
            value=CURRENT_SETTINGS["temperature"],
            interactive=True,
            label="Temperature",
            info="Temperature controls the ‚Äúcreativity‚Äù or randomness of the text generated by the model.",
        )
        model_name = gr.Dropdown(
            ["gpt-3.5-turbo", "text-davinci-003", "text-curie-001"],
            label="OpenAI Model",
            interactive=True,
            value=CURRENT_SETTINGS["model_name"],
            info="OpenAI Model to use gpt-3.5-turbo	is recommended",
        )
        max_tokens = gr.Slider(
            minimum=512,
            maximum=4096,
            interactive=True,
            label="Max Tokens",
            value=CURRENT_SETTINGS["max_tokens"],
            info="Maximum number of tokens to generate",
        )
        timeout_seconds = gr.Slider(
            minimum=10,
            maximum=60,
            interactive=True,
            step=5,
            label="Timeout",
            info="Timeout period in seconds for OpenAI requests",
            value=CURRENT_SETTINGS["timeout"],
        )
        max_retries = gr.Slider(
            minimum=0,
            maximum=10,
            interactive=True,
            step=1,
            label="Max Retries",
            info="Number of times to retry the LLM inference requests sent to OpenAI",
            value=CURRENT_SETTINGS["max_retries"],
        )
        reset_to_defaults = gr.Checkbox(label="Reset to Defaults", interactive=True, info="Check this box to reset to default settings.")
        apply_settings = gr.Button("Apply Settings")

        apply_settings.click(
            reset_llm_chain,
            inputs=[
                model_name,
                temperature,
                max_tokens,
                timeout_seconds,
                max_retries,
                reset_to_defaults,
            ],
            outputs=[current_settings],
        )

demo.launch(
    show_api=False,
    auth=(os.getenv("APP_USER"), os.getenv("APP_PASSWORD")),
    server_name=os.getenv("APP_HOST"),
    server_port=int(os.getenv("APP_PORT")),
)
